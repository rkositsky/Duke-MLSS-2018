{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Kevin Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. One of the fundamental challenges of reinforcement learning is balancing *exploration* versus *exploitation*. What do these two terms mean, and why do they present a challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Exploration: Gathering more information about the environment; in other words, searching the policy space for potentially better (higher reward) solutions than what the agent currently knows.`\n",
    "\n",
    "`Exploitation: Following the best policy already learned, with the goal of maximizing total reward.`\n",
    "\n",
    "`The goal of the RL agent is to maximize the total reward it can receive throughout its lifetime. It may find certain solutions first, but these may be local optima--i.e. there may be better solutions out there. However, searching (exploring) for better solutions will necessitate trying out many bad trajectories, which decreases the agents total reward in the short term. Too much exploration, on the other hand, might mean the agent wastes actions on moves it believes have lower payoff.\n",
    "In order to maximize the reward gained, it needs to also follow the policy it's learned (exploiting).`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Another fundamental reinforcement learning challenge is what is known as the *credit assignment problem*, especially when rewards are sparse. \n",
    "What do we mean by the phrase, and why does this make learning especially difficult?\n",
    "How does this interact with reward function design, where we have to be careful that our reward captures the true objective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`If rewards are sparse or don't come until the end of an episode (e.g. winning or losing a game), then the agent will have had to navigate many states and actions before receiving any feedback. In such a situation, it is hard to tell which decisions led to final good (or bad) result. This makes learning difficult, as we can't properly assign credit for the reward to each decision.`\n",
    "\n",
    "`For example, a chess bot can play like a grandmaster, but then make a single boneheaded move that leads to it being checkmated. Despite all the good moves it made before the checkmate, the entire trajectory is now credited with a loss.`\n",
    "\n",
    "`However, designing a good reward function that provides good feedback frequently is often difficult or impossible, as it may interfere with the reward function capturing the true objective. As such, we often have to make a trade-off between the credit assignment problem and having an accurate reward function.`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep SARSA Cart Pole\n",
    "\n",
    "[SARSA (state-action-reward-state-action)](https://en.wikipedia.org/wiki/Stateâ€“actionâ€“rewardâ€“stateâ€“action) is another Q value algorithm that resembles Q-learning quite closely:\n",
    "\n",
    "Q-learning update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma \\max_a Q_\\pi(s_{t+1}, a)\\big)\n",
    "\\end{equation}\n",
    "\n",
    "SARSA update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma Q_\\pi(s_{t+1}, a_{t+1})\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Unlike Q-learning, which is considered an *off-policy* network, SARSA is an *on-policy* algorithm. \n",
    "When Q-learning calculates the estimated future reward, it must \"guess\" the future, starting with the next action the agent will take. In Q-learning, we assume the agent will take the best possible action: $\\max_a Q_\\pi(s_{t+1}, a)$. SARSA, on the other hand, uses the action that was actually taken next in the episode we are learning from: $Q_\\pi(s_{t+1}, a_{t+1})$. In other words, SARSA learns from the next action he actually took (on policy), as opposed to what the max possible Q value for the next state was (off policy).\n",
    "\n",
    "Build an RL agent that uses SARSA to solve the Cart Pole problem. \n",
    "\n",
    "*Hint: You can and should reuse the Q-Learning agent we went over earlier. In fact, if you know what you're doing, it's possible to finish this assignment in about 30 seconds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin_000\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 14.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 13.9 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 51.37 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 114.1 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 192.14 ticks.\n",
      "Ran 404 episodes. Solved after 304 trials âœ”\n"
     ]
    }
   ],
   "source": [
    "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    ## CHANGES HERE (add next_action as a parameter)\n",
    "    def remember(self, state, action, reward, next_state, next_action, done):\n",
    "        # ... and make sure to append the next_action to our memory.\n",
    "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        return self.env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "\n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        ## CHANGES HERE: pull out next_action (along with other recorded data)\n",
    "         # as we loop through the memories in our minibatch.\n",
    "        for state, action, reward, next_state, next_action, done in minibatch:\n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "            ## CHANGES HERE: rather than take the maximum value in the Q-value vector,\n",
    "            ## we pull out the value corresponding with the action we actually took,\n",
    "            ## by indexing the vector at [next_action].\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state})[0][next_action]\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0])\n",
    "        \n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            ## CHANGES HERE: We calculate an initial action\n",
    "            ## outside the loop, so that as we play Cartpole,\n",
    "            ## we are operating at a one-time-step \"lag\": we can then\n",
    "            ## record (previous_state, previous_action, reward, new_state, new_action)\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            action = self.choose_action(state, self.get_epsilon(e))\n",
    "            done = False\n",
    "            i = 0\n",
    "            while not done:\n",
    "                if e % 100 == 0 and not self.quiet:\n",
    "                    self.env.render()\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                ## CHANGES HERE: compute and store the *next* action we will take,\n",
    "                ## so that later, when replaying our memories, we will know not just\n",
    "                ## the action we took in a given state, but also the next action.\n",
    "                next_action = self.choose_action(next_state, self.get_epsilon(e))\n",
    "                self.remember(state, action, reward, next_state, next_action, done)\n",
    "                state = next_state\n",
    "                ## CHANGES HERE: at the end of the loop, we set our last action to\n",
    "                ## the next action we've computed, preparing us for the next iteration.\n",
    "                action = next_action\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should be able to find that SARSA works much better for the demo we went over during lecture.\n",
    "This is not necessarily a general result.\n",
    "Q-learning and SARSA tend to do better on different kinds of problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
