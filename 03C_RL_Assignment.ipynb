{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Reinforcement Learning (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
    "Name: Rachel Kositsky"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "- Why don't we multiply by alpha when updating the action Q value? \n",
    "- Answer: because we're using the continuous neural network, and it updates via backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short answer\n",
    "\n",
    "1\\. One of the fundamental challenges of reinforcement learning is balancing *exploration* versus *exploitation*. What do these two terms mean, and why do they present a challenge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network has the task of maximizing its reward on a task. It does this by taking the best action at any given state according to its policy. However, this way it can become fixed in a certain policy which may not be the optimal policy. _Exploration_ is the idea of taking potentially non-optimal actions in order to improve the policy. _Exploitation_ is the idea of using the learned policy to maximize the reward. \n",
    "\n",
    "No exploration means that the learned policy may converge on a non-optimal solution. No exploitation means that the rewards will be non-optimal because the rewards per state will never have been chosen by the policy. It is a challenge to balance these two in a way that allows learning and accurate reward propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Another fundamental reinforcement learning challenge is what is known as the *credit assignment problem*, especially when rewards are sparse. \n",
    "What do we mean by the phrase, and why does this make learning especially difficult?\n",
    "How does this interact with reward function design, where we have to be careful that our reward captures the true objective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _credit assigning problem_ is that rewards for achieving the overall goal may only occur after many steps, decreasing the reward and making it difficult to update beneficial steps. This often occurs when the \"win\" outcome is only one state in a continuous or large discrete state space. However, reward functions still have to be designed to capture the true objective and not intermediate goals, as the model may learn to achieve the intermediate goals as the expense of the true objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep SARSA Cart Pole\n",
    "\n",
    "[SARSA (state-action-reward-state-action)](https://en.wikipedia.org/wiki/Stateâ€“actionâ€“rewardâ€“stateâ€“action) is another Q value algorithm that resembles Q-learning quite closely:\n",
    "\n",
    "Q-learning update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma \\max_a Q_\\pi(s_{t+1}, a)\\big)\n",
    "\\end{equation}\n",
    "\n",
    "SARSA update rule:\n",
    "\\begin{equation}\n",
    "Q_\\pi (s_t, a_t) \\leftarrow (1 - \\alpha) \\cdot Q_\\pi(s_t, a_t) + \\alpha \\cdot \\big(r_t + \\gamma Q_\\pi(s_{t+1}, a_{t+1})\\big)\n",
    "\\end{equation}\n",
    "\n",
    "Unlike Q-learning, which is considered an *off-policy* network, SARSA is an *on-policy* algorithm. \n",
    "When Q-learning calculates the estimated future reward, it must \"guess\" the future, starting with the next action the agent will take. In Q-learning, we assume the agent will take the best possible action: $\\max_a Q_\\pi(s_{t+1}, a)$. SARSA, on the other hand, uses the action that was actually taken next in the episode we are learning from: $Q_\\pi(s_{t+1}, a_{t+1})$. In other words, SARSA learns from the next action he actually took (on policy), as opposed to what the max possible Q value for the next state was (off policy).\n",
    "\n",
    "Build an RL agent that uses SARSA to solve the Cart Pole problem. \n",
    "\n",
    "*Hint: You can and should reuse the Q-Learning agent we went over earlier. In fact, if you know what you're doing, it's possible to finish this assignment in about 30 seconds.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[Episode 0] - Mean survival time over last 100 episodes was 13.0 ticks.\n",
      "[Episode 100] - Mean survival time over last 100 episodes was 27.3 ticks.\n",
      "[Episode 200] - Mean survival time over last 100 episodes was 58.06 ticks.\n",
      "[Episode 300] - Mean survival time over last 100 episodes was 178.13 ticks.\n",
      "[Episode 400] - Mean survival time over last 100 episodes was 97.01 ticks.\n",
      "[Episode 500] - Mean survival time over last 100 episodes was 140.76 ticks.\n",
      "[Episode 600] - Mean survival time over last 100 episodes was 169.74 ticks.\n",
      "[Episode 700] - Mean survival time over last 100 episodes was 183.58 ticks.\n",
      "[Episode 800] - Mean survival time over last 100 episodes was 164.75 ticks.\n",
      "[Episode 900] - Mean survival time over last 100 episodes was 158.28 ticks.\n",
      "Did not solve after 999 episodes ðŸ˜ž\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE ###\n",
    "# Based on: https://gym.openai.com/evaluations/eval_EIcM1ZBnQW2LBaFN6FY65g/\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "\n",
    "class DQNCartPoleSolver():\n",
    "    def __init__(self, n_episodes=1000, n_win_ticks=195, max_env_steps=None, gamma=1.0, epsilon=1.0, epsilon_min=0.01, epsilon_log_decay=0.995, alpha=0.01, alpha_decay=0.01, batch_size=64, monitor=False, quiet=False):\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.env = gym.make('CartPole-v0')\n",
    "        if monitor: self.env = gym.wrappers.Monitor(self.env, '../data/cartpole-1', force=True)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_log_decay\n",
    "        self.alpha = alpha\n",
    "        self.alpha_decay = alpha_decay\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_win_ticks = n_win_ticks\n",
    "        self.batch_size = batch_size\n",
    "        self.quiet = quiet\n",
    "        if max_env_steps is not None: self.env._max_episode_steps = max_env_steps\n",
    "\n",
    "        # Init model\n",
    "        self.state_ = tf.placeholder(tf.float32, shape=[None, 4])\n",
    "        h = tf.layers.dense(self.state_, units=24, activation=tf.nn.tanh)\n",
    "        h = tf.layers.dense(h, units=48, activation=tf.nn.tanh)\n",
    "        self.Q = tf.layers.dense(h, units=2)\n",
    "        \n",
    "        self.Q_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "        loss = tf.losses.mean_squared_error(self.Q_, self.Q)\n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        lr = tf.train.exponential_decay(0.01, self.global_step, 0.995, 1)\n",
    "        self.train_step = tf.train.AdamOptimizer(lr).minimize(loss, global_step=self.global_step)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, next_action, done):\n",
    "        self.memory.append((state, action, reward, next_state, next_action, done))\n",
    "\n",
    "    def choose_action(self, state, epsilon):\n",
    "        if (np.random.random() <= epsilon):\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.sess.run(self.Q, feed_dict={self.state_: state}))\n",
    "        return action\n",
    "        \n",
    "    def get_epsilon(self, t):\n",
    "        return max(self.epsilon_min, min(self.epsilon, 1.0 - math.log10((t + 1) * self.epsilon_decay)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return np.reshape(state, [1, 4])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # TODO: change the update here. gets called after each episode ends.\n",
    "        x_batch, y_batch = [], []\n",
    "        minibatch = random.sample(\n",
    "            self.memory, min(len(self.memory), batch_size))\n",
    "        \n",
    "        # can you see it in minibatch?\n",
    "        # y_target: initialized as .Q\n",
    "        # y_target: is the avg reward?\n",
    "        for state, action, reward, next_state, next_action, done in minibatch:\n",
    "            y_target = self.sess.run(self.Q, feed_dict={self.state_: state})\n",
    "            \n",
    "            # Change the update for the action to SARSA update\n",
    "            y_target[0][action] = reward if done else reward + self.gamma * self.sess.run(self.Q, feed_dict={self.state_: next_state})[0][next_action]\n",
    "            x_batch.append(state[0])\n",
    "            y_batch.append(y_target[0]) \n",
    "        self.sess.run(self.train_step, feed_dict={self.state_: np.array(x_batch), self.Q_: np.array(y_batch)})\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def run(self):\n",
    "        scores = deque(maxlen=100)\n",
    "\n",
    "        for e in range(self.n_episodes):\n",
    "            state = self.preprocess_state(self.env.reset())\n",
    "            done = False\n",
    "            i = 0\n",
    "            next_action = self.choose_action(state, self.get_epsilon(e))\n",
    "            \n",
    "            while not done:\n",
    "                if e % 100 == 0 and not self.quiet:\n",
    "                    self.env.render()\n",
    "                action = next_action\n",
    "                #action = self.choose_action(state, self.get_epsilon(e))\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                next_state = self.preprocess_state(next_state)\n",
    "                next_action = self.choose_action(state, self.get_epsilon(e))\n",
    "                self.remember(state, action, reward, next_state, next_action, done)\n",
    "                state = next_state\n",
    "                i += 1\n",
    "\n",
    "            scores.append(i)\n",
    "            mean_score = np.mean(scores)\n",
    "            if mean_score >= self.n_win_ticks and e >= 100:\n",
    "                if not self.quiet: print('Ran {} episodes. Solved after {} trials âœ”'.format(e, e - 100))\n",
    "                return e - 100\n",
    "            if e % 100 == 0 and not self.quiet:\n",
    "                print('[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'.format(e, mean_score))\n",
    "\n",
    "            self.replay(self.batch_size)\n",
    "        \n",
    "        if not self.quiet: print('Did not solve after {} episodes ðŸ˜ž'.format(e))\n",
    "        return e\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    agent = DQNCartPoleSolver()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you should be able to find that SARSA works much better for the demo we went over during lecture.\n",
    "This is not necessarily a general result.\n",
    "Q-learning and SARSA tend to do better on different kinds of problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
