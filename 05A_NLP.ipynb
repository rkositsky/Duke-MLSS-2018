{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP) in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings, or word vectors, provide a way of mapping words from a vocabulary into a low-dimensional space, where words with similar meanings are close together. Let's play around with a set of pre-trained word vectors, to get used to their properties. There exist many sets of pretrained word embeddings; here, we use ConceptNet Numberbatch, which provides a relatively small download in an easy-to-work-with format (h5).\n",
    "\n",
    "Note from class: Why wouldn't we just want to put words in a big dictionary?<br>\n",
    "A: You may give implicit relationship between words that are close to each other in the dictionary.\n",
    "\n",
    "Q: Why not one-hot encoding?<br>\n",
    "A: You're not reducing dimensionality here. You're also ruining any chance of learning relationships between words - \"toad\", \"frog\", and \"toaster\" will all be orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download word vectors => did through wget\n",
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "if not os.path.isfile('mini.h5'):\n",
    "    print(\"Downloading Conceptnet Numberbatch word embeddings...\")\n",
    "    conceptnet_url = 'http://conceptnet.s3.amazonaws.com/precomputed-data/2016/numberbatch/17.06/mini.h5'\n",
    "    urlretrieve(conceptnet_url, 'mini.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read an `h5` file, we'll need to use the `h5py` package. Below, we use the package to open the `mini.h5` file we just downloaded. We extract from the file a list of utf-8-encoded words, as well as their $300$-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_words dimensions: 362891\n",
      "all_embeddings dimensions: (362891, 300)\n",
      "/c/de/aufmachung\n"
     ]
    }
   ],
   "source": [
    "# Load the file and pull out words and embeddings\n",
    "import h5py\n",
    "\n",
    "with h5py.File(\"mini.h5\", \"r\") as f:\n",
    "    all_words = [word.decode('utf-8') for word in f['mat']['axis1'][:]]\n",
    "    all_embeddings = f['mat']['block0_values'][:]\n",
    "    \n",
    "print(\"all_words dimensions: {0}\".format(len(all_words)))\n",
    "print(\"all_embeddings dimensions: {0}\".format(all_embeddings.shape))\n",
    "\n",
    "print(all_words[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `all_words` is a list of $V$ strings (what we call our *vocabulary*), and `all_embeddings` is a $V \\times 300$ matrix. The strings are of the form `/c/language_code/word`â€”for example, `/c/en/cat` and `/c/es/gato`.\n",
    "\n",
    "We are interested only in the English words. We use Python list comprehensions to pull out the indices of the English words, then extract just the English words (stripping the six-character `/c/en/` prefix) and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english_words dimensions: 150875\n",
      "english_embeddings dimensions: (150875, 300)\n",
      "activated_carbon\n"
     ]
    }
   ],
   "source": [
    "# Restrict our vocabulary to just the English words\n",
    "# first 6 are /c/en\n",
    "english_words = [word[6:] for word in all_words if word.startswith('/c/en')]\n",
    "english_word_indices = [i for i, word in enumerate(all_words) if word.startswith('/c/en')]\n",
    "english_embeddings = all_embeddings[english_word_indices]\n",
    "\n",
    "print(\"english_words dimensions: {0}\".format(len(english_words)))\n",
    "print(\"english_embeddings dimensions: {0}\".format(english_embeddings.shape))\n",
    "\n",
    "print(english_words[1337])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magnitude of a word vector is less important than its direction; the magnitude can be thought of as representing frequency of use, independent of the semantics of the word. \n",
    "Here, we will be interested in semantics, so we *normalize* our vectors, dividing each by its length. \n",
    "The result is that all of our word vectors are length 1, and as such, lie on a unit circle. \n",
    "The dot product of two vectors is proportional to the cosine of the angle between them, and provides a measure of similarity (the bigger the cosine, the smaller the angle).\n",
    "\n",
    "<img src=\"Figures/cosine_similarity.png\" alt=\"cosine\" style=\"width: 500px;\"/>\n",
    "<center>Figure adapted from *[Mastering Machine Learning with Spark 2.x](https://www.safaribooksonline.com/library/view/mastering-machine-learning/9781785283451/ba8bef27-953e-42a4-8180-cea152af8118.xhtml)*</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_embeddings dimensions: (150875, 300)\n",
      "[[57.5326  ]\n",
      " [57.29747 ]\n",
      " [57.21014 ]\n",
      " ...\n",
      " [57.384666]\n",
      " [56.97368 ]\n",
      " [57.471733]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "norms = np.linalg.norm(english_embeddings, axis = 1)\n",
    "normalized_embeddings = english_embeddings.astype('float32') / norms.astype('float32').reshape([-1, 1])\n",
    "\n",
    "print(\"normalized_embeddings dimensions: {0}\".format(normalized_embeddings.shape))\n",
    "print(norms.astype('float32').reshape([-1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look up words easily, so we create a dictionary that maps us from a word to its index in the word embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99999994\n",
      "1330\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "index = {word: i for i, word in enumerate(english_words)}\n",
    "print(np.dot(normalized_embeddings[0,:], normalized_embeddings[0,:]))\n",
    "print(index[\"actionable\"])\n",
    "print(np.dot(normalized_embeddings[1330,:], normalized_embeddings[1330,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to measure the similarity between pairs of words. We use numpy to take dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\tcat\t 1.0\n",
      "cat\tfeline\t 0.81995475\n",
      "cat\tdog\t 0.59072405\n",
      "cat\tmoo\t 0.0039538294\n",
      "cat\tfreeze\t -0.030225188\n",
      "antonyms\topposites\t 0.39410648\n",
      "antonyms\tsynonyms\t 0.46883985\n"
     ]
    }
   ],
   "source": [
    "def similarity_score(w1, w2):\n",
    "    score = np.dot(normalized_embeddings[index[w1],:], normalized_embeddings[index[w2],:])\n",
    "    return score\n",
    "\n",
    "# # A word is as similar with itself as possible:\n",
    "print('cat\\tcat\\t', similarity_score('cat', 'cat'))\n",
    "\n",
    "# # Closely related words still get high scores:\n",
    "print('cat\\tfeline\\t', similarity_score('cat', 'feline'))\n",
    "print('cat\\tdog\\t', similarity_score('cat', 'dog'))\n",
    "\n",
    "# # Unrelated words, not so much\n",
    "print('cat\\tmoo\\t', similarity_score('cat', 'moo'))\n",
    "print('cat\\tfreeze\\t', similarity_score('cat', 'freeze'))\n",
    "\n",
    "# # Antonyms are still considered related, sometimes more so than synonyms\n",
    "print('antonyms\\topposites\\t', similarity_score('antonym', 'opposite'))\n",
    "print('antonyms\\tsynonyms\\t', similarity_score('antonym', 'synonym'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also find, for instance, the most similar words to a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_to_vector(v, n):\n",
    "    \"\"\"Return n closest words to vector v\"\"\"\n",
    "    all_scores = np.dot(normalized_embeddings, v)\n",
    "    best_words = map(lambda i: english_words[i], reversed(np.argsort(all_scores)))\n",
    "    return [next(best_words) for _ in range(n)]\n",
    "\n",
    "def most_similar(w, n):\n",
    "    \"\"\"Return n closest words to word w\"\"\"\n",
    "    return closest_to_vector(normalized_embeddings[index[w],:], n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'humane_society', 'kitten', 'feline', 'colocolo', 'cats', 'kitty', 'maine_coon', 'housecat', 'sharp_teeth']\n",
      "['dog', 'dogs', 'wire_haired_dachshund', 'doggy_paddle', 'lhasa_apso', 'good_friend', 'puppy_dog', 'bichon_frise', 'woof_woof', 'golden_retrievers']\n",
      "['duke', 'dukes', 'duchess', 'duchesses', 'ducal', 'dukedom', 'duchy', 'voivode', 'princes', 'prince']\n"
     ]
    }
   ],
   "source": [
    "print(most_similar(\"cat\", 10))\n",
    "print(most_similar(\"dog\", 10))\n",
    "print(most_similar(\"duke\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `closest_to_vector` to find words \"nearby\" vectors that we create ourselves. This allows us to solve analogies. For example, in order to solve the analogy \"man : brother :: woman : ?\", we can compute a new vector `brother - man + woman`: the meaning of brother, minus the meaning of man, plus the meaning of woman. We can then ask which words are closest, in the embedding space, to that new vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sister', 'brother', 'sisters', 'kid_sister', 'younger_brother', 'niece', 'nieces', 'sistren', 'stepsister', 'daughter']\n",
      "['sister', 'brother', 'sisters', 'kid_sister', 'younger_brother', 'niece', 'nieces', 'sistren', 'stepsister', 'daughter']\n",
      "['man', 'mans', 'woman', 'men', 'dude', 'brotherman', 'bloke', 'peter_pan', 'guy', 'mandem']\n",
      "['paris', 'france', 'le_havre', 'in_france', 'montmartre', 'marseille', 'loire_valley', 'saone', 'lyonnais', 'jacques_chirac']\n"
     ]
    }
   ],
   "source": [
    "def solve_analogy(a1, b1, a2):\n",
    "    \"\"\"Solve the analogy of a1 : b1 :: a2 : ?\"\"\"\n",
    "    b2_vector = normalized_embeddings[index[b1],:] - normalized_embeddings[index[a1],:] + normalized_embeddings[index[a2],:]\n",
    "    b2 = closest_to_vector(b2_vector, 10)\n",
    "    return b2\n",
    "    \n",
    "print(solve_analogy(\"man\", \"brother\", \"woman\"))\n",
    "print(solve_analogy(\"man\", \"woman\", \"brother\"))\n",
    "print(solve_analogy(\"king\", \"man\", \"queen\")) # didn't work as well...\n",
    "print(solve_analogy(\"spain\", \"madrid\", \"france\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These three results are quite good, but in general, the results of these analogies can be disappointing. Try experimenting with other analogies, and see if you can think of ways to get around the problems you notice (i.e., modifications to the solve_analogy algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using word embeddings in deep models\n",
    "Word embeddings are fun to play around with, but their primary use is that they allow us to think of words as existing in a continuous, Euclidean space; we can then use an existing arsenal of techniques for machine learning with continuous numerical data (like logistic regression or neural networks) to process text.\n",
    "\n",
    "Let's take a look at an especially simple version of this. We'll perform *sentiment analysis* on a set of movie reviews: in particular, we will attempt to classify a movie review as positive or negative based on its text.\n",
    "\n",
    "We will use a [Simple Word Embedding Model](http://people.ee.duke.edu/~lcarin/acl2018_swem.pdf) (SWEM, Shen et al. 2018) to do so. We will represent a review as the *mean* of the embeddings of the words in the review. Then we'll train a three-layer MLP (a neural network) to classify the review as positive or negative.\n",
    "\n",
    "Download the `movie-simple.txt` file from Google Classroom into this directory. Each line of that file contains \n",
    "\n",
    "1. the numeral 0 (for negative) or the numeral 1 (for positive), followed by\n",
    "2. a tab (the whitespace character), and then\n",
    "3. the review itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "remove_punct=str.maketrans('','',string.punctuation)\n",
    "\n",
    "# This function converts a line of our data file into\n",
    "# a tuple (x, y), where x is 300-dimensional representation\n",
    "# of the words in a review, and y is its label.\n",
    "def convert_line_to_example(line):\n",
    "    # Pull out the first character: that's our label (0 or 1)\n",
    "    y = int(line[0])\n",
    "    \n",
    "    # Split the line into words using Python's split() function\n",
    "    words = line[2:].translate(remove_punct).lower().split()\n",
    "    \n",
    "    # Look up the embeddings of each word, ignoring words not\n",
    "    # in our pretrained vocabulary.\n",
    "    embeddings = [normalized_embeddings[index[w]] for w in words\n",
    "                  if w in index]\n",
    "    \n",
    "    # Take the mean of the embeddings\n",
    "    x = np.mean(np.vstack(embeddings), axis=0)\n",
    "    return {'x': x, 'y': y}\n",
    "\n",
    "# Apply the function to each line in the file.\n",
    "with open(\"movie-simple.txt\", \"r\", encoding='utf-8', errors='ignore') as f:\n",
    "    dataset = [convert_line_to_example(l) for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews in dataset: 1411\n",
      "Each one is a dictionary: {x: [300 long average embedding], y: label}\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of reviews in dataset: {}\".format(len(dataset)))\n",
    "print(\"Each one is a dictionary: {x: [300 long average embedding], y: label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a dataset, let's shuffle it and do a train/test split. We use a quarter of the dataset for testing, 3/4 for training (but also ensure that we have a whole number of batches in our training set, to make the code nicer later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(dataset)\n",
    "\n",
    "batch_size = 100\n",
    "total_batches = len(dataset) // batch_size \n",
    "train_batches = 3*total_batches // 4\n",
    "train, test = dataset[:train_batches*batch_size], dataset[train_batches*batch_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 // 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to build our MLP in Tensorflow. We'll use placeholders for `X` and `y` as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Placeholders for input\n",
    "X = tf.placeholder(tf.float32, [None, 300])\n",
    "y = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# Three-layer MLP\n",
    "h1 = tf.layers.dense(X, 100, tf.nn.relu)\n",
    "h2 = tf.layers.dense(h1, 20, tf.nn.relu)\n",
    "logits = tf.layers.dense(h2, 1)\n",
    "# Note: do not apply nonlinearity to logits before softmax, because softmax is already nonlinear\n",
    "probabilities = tf.sigmoid(logits)\n",
    "\n",
    "# Loss and metrics\n",
    "# not softmax, but this fxn is same but only for one class\n",
    "# reduce_mean: average across all samples in the minibatch\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = y))\n",
    "# Round probabilities => get 0,1 prediction\n",
    "# Compare to labels y\n",
    "# Cast to float and get average accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(probabilities), y), tf.float32))\n",
    "\n",
    "# Training\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\n",
    "# Initialization of variables\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now begin a session and train our model. We'll train for 250 epochs. When we're finished, we'll evaluate our accuracy on all the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tLoss: 0.69\tAccuracy: 0.60\n",
      "Epoch: 10\tLoss: 0.67\tAccuracy: 0.60\n",
      "Epoch: 20\tLoss: 0.66\tAccuracy: 0.59\n",
      "Epoch: 30\tLoss: 0.65\tAccuracy: 0.62\n",
      "Epoch: 40\tLoss: 0.66\tAccuracy: 0.53\n",
      "Epoch: 50\tLoss: 0.61\tAccuracy: 0.70\n",
      "Epoch: 60\tLoss: 0.59\tAccuracy: 0.75\n",
      "Epoch: 70\tLoss: 0.51\tAccuracy: 0.84\n",
      "Epoch: 80\tLoss: 0.45\tAccuracy: 0.88\n",
      "Epoch: 90\tLoss: 0.39\tAccuracy: 0.88\n",
      "Epoch: 100\tLoss: 0.34\tAccuracy: 0.93\n",
      "Epoch: 110\tLoss: 0.26\tAccuracy: 0.91\n",
      "Epoch: 120\tLoss: 0.20\tAccuracy: 0.97\n",
      "Epoch: 130\tLoss: 0.22\tAccuracy: 0.90\n",
      "Epoch: 140\tLoss: 0.18\tAccuracy: 0.96\n",
      "Epoch: 150\tLoss: 0.16\tAccuracy: 0.96\n",
      "Epoch: 160\tLoss: 0.18\tAccuracy: 0.91\n",
      "Epoch: 170\tLoss: 0.15\tAccuracy: 0.96\n",
      "Epoch: 180\tLoss: 0.11\tAccuracy: 0.96\n",
      "Epoch: 190\tLoss: 0.08\tAccuracy: 0.98\n",
      "Epoch: 200\tLoss: 0.17\tAccuracy: 0.92\n",
      "Epoch: 210\tLoss: 0.10\tAccuracy: 0.97\n",
      "Epoch: 220\tLoss: 0.14\tAccuracy: 0.94\n",
      "Epoch: 230\tLoss: 0.10\tAccuracy: 0.96\n",
      "Epoch: 240\tLoss: 0.10\tAccuracy: 0.98\n",
      "Final test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "sess = tf.Session()\n",
    "sess.run(init_op)\n",
    "\n",
    "for epoch in range(250):\n",
    "    for batch in range(train_batches):\n",
    "        data = train[batch*batch_size:(batch+1)*batch_size]\n",
    "        reviews = [sample['x'] for sample in data]\n",
    "        labels  = [sample['y'] for sample in data]\n",
    "        labels = np.array(labels).reshape([-1, 1])\n",
    "    \n",
    "        _, l, acc = sess.run([train_step, loss, accuracy], feed_dict={X: reviews, y: labels})\n",
    "        \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {0}\\tLoss: {1:1.2f}\\tAccuracy: {2:1.2f}\".format(epoch, l, acc))    \n",
    "    random.shuffle(train)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_reviews = [sample['x'] for sample in test]\n",
    "test_labels  = [sample['y'] for sample in test]\n",
    "test_labels = np.array(test_labels).reshape([-1, 1])\n",
    "acc = sess.run(accuracy, feed_dict={X: test_reviews, y: test_labels})\n",
    "\n",
    "print(\"Final test accuracy: {0:1.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine what our model has learned, seeing how it responds to word vectors for different words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exciting [[0.99997854]]\n",
      "hated [[1.1961651e-08]]\n",
      "boring [[1.2153959e-06]]\n",
      "loved [[0.99999964]]\n",
      "okay [[0.03028856]]\n",
      "all_right [[0.9836664]]\n"
     ]
    }
   ],
   "source": [
    "# Check some words\n",
    "words_to_test = [\"exciting\", \"hated\", \"boring\", \"loved\", \"okay\", \"all_right\"]\n",
    "\n",
    "for word in words_to_test:\n",
    "    print(\"{0} {1}\".format(word, sess.run(probabilities, feed_dict={X: normalized_embeddings[index[word]].reshape(1, 300)})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try some words of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model works great for such a simple dataset, but does a little less well on something more complex. `movie-pang02.txt`, for instance, has 2000 longer, more complex movie reviews. It's in the same format as our simple dataset. On those longer reviews, this model achieves only 60-80% accuracy. (Increasing the number of epochs to, say, 1000, does help.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks (RNNs)\n",
    "\n",
    "In the context of deep learning, natural language is commonly modeled with Recurrent Neural Networks (RNNs).\n",
    "RNNs pass the output of a neuron back to the input of the next time step of the same neuron.\n",
    "These directed cycles in the RNN architecture gives them the ability to model temporal dynamics, making them particularly suited for modeling sequences (e.g. text).\n",
    "We can visualize an RNN layer as follows:\n",
    "\n",
    "<img src=\"Figures/basic_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 80px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "We can unroll an RNN through time, making the sequence aspect of them more obvious:\n",
    "\n",
    "<img src=\"Figures/unrolled_RNN.PNG\" alt=\"basic_RNN\" style=\"width: 400px;\"/>\n",
    "<center>Figure from *Understanding LSTMs*. https://colah.github.io/posts/2015-08-Understanding-LSTMs/</center>\n",
    "\n",
    "#### RNNs in TensorFlow\n",
    "How would we implement an RNN in TensorFlow? Given the different forms of RNNs, there are quite a few ways, but we'll stick to a simple one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As always, import TensorFlow first\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have our inputs in word embedding form already, say of dimensionality 100. We'll use a minibatch size of 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = 16\n",
    "x_dim = 100\n",
    "\n",
    "# Inputs\n",
    "x1 = tf.placeholder(tf.float32, [mb, x_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define weight matrices for projecting the input, the previous state, and the output. Rather arbitrarily, let's pick a hidden layer size of 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 64\n",
    "\n",
    "# For projecting the input\n",
    "U = tf.Variable(tf.truncated_normal([x_dim, h_dim], stddev=0.1))\n",
    "\n",
    "# For projecting the previous state, which is already 64x64\n",
    "W = tf.Variable(tf.truncated_normal([h_dim, h_dim], stddev=0.1))\n",
    "\n",
    "# For projecting the output back to x_dim\n",
    "V = tf.Variable(tf.truncated_normal([h_dim, x_dim], stddev=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a function for one time step of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_step(x, h):\n",
    "    # add new input + memory from previous state\n",
    "    h_next = tf.tanh(tf.matmul(x, U) + tf.matmul(h, W))\n",
    "    \n",
    "    # Project from hidden dimension to output\n",
    "    output = tf.matmul(h_next, V)\n",
    "    \n",
    "    return output, h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y1 dimensions: (16, 100)\n",
      "Hidden state h1 dimensions: (16, 64)\n"
     ]
    }
   ],
   "source": [
    "# Initialize hidden state to 0\n",
    "h0 = tf.zeros([mb, h_dim])\n",
    "\n",
    "# Forward pass of one RNN step for time step t=1\n",
    "y1, h1 = RNN_step(x1, h0)\n",
    "\n",
    "print(\"Output y1 dimensions: {0}\".format(y1.shape))\n",
    "print(\"Hidden state h1 dimensions: {0}\".format(h1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat using the `RNN_step` function to continue unrolling the RNN as far as we need to. For each step, we feed in the next input (a new placeholder) and get a new output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output y2 dimensions: (16, 100)\n",
      "Hidden state h2 dimensions: (16, 64)\n"
     ]
    }
   ],
   "source": [
    "x2 = tf.placeholder(tf.float32, [mb, x_dim]) # word 2\n",
    "\n",
    "# Forward pass of one RNN step for time step t=2\n",
    "y2, h2 = RNN_step(x2, h1)\n",
    "\n",
    "print(\"Output y2 dimensions: {0}\".format(y2.shape))\n",
    "print(\"Hidden state h2 dimensions: {0}\".format(h2.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in practice, you'd want to do this unrolling with a `for` loop, and the RNN functionality is more cleanly wrapped up in a class. \n",
    "We're not going to implement the class version here though, as TensorFlow already has these implemented: https://www.tensorflow.org/api_guides/python/contrib.rnn#Base_interface_for_all_RNN_Cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x dimensions:\n",
      "[TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)]), TensorShape([Dimension(16), Dimension(100)])]\n",
      "\n",
      "h dimensions:\n",
      "[TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)]), TensorShape([Dimension(16), Dimension(64)])]\n"
     ]
    }
   ],
   "source": [
    "# Number of steps to unroll\n",
    "num_steps = 10\n",
    "\n",
    "# List of inputs and hidden states\n",
    "xs = []\n",
    "hs = []\n",
    "\n",
    "# Build RNN\n",
    "rnn = tf.contrib.rnn.BasicRNNCell(h_dim)\n",
    "\n",
    "# Initialize hidden state to zero\n",
    "h_t = tf.zeros([mb, h_dim])\n",
    "    \n",
    "for t in range(num_steps):\n",
    "    x_t = tf.placeholder(tf.float32, [mb, x_dim])\n",
    "    h_t, _ = rnn(x_t, h_t)\n",
    "    \n",
    "    xs.append(x_t)\n",
    "    hs.append(h_t)\n",
    "\n",
    "print(\"x dimensions:\")\n",
    "print([x_t.shape for x_t in xs])\n",
    "print(\"\\nh dimensions:\")\n",
    "print([h_t.shape for h_t in hs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: people don't actually use RNNs this way. Because of backpropagation, the gradients might vanish over time => vanishing gradient problem. LSTMs are a way around that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short-Term Memory (LSTM)\n",
    "One popular type of RNNs are Long Short-Term Memory (LSTM) networks.\n",
    "We're not going to go into detail here about what structural differences they have from vanilla RNNs, but LSTMs are also sequence modeling neural networks, with much better long range model capabilities.\n",
    "If you're curious, [this](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) does a fantastic job describing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other materials:\n",
    "Like Reinforcement Learning, Natural Language Processing can also easily be several full courses on its own at most universities, both with or without neural networks.\n",
    "In fact, Prof Mohit Bansal has [taught](http://www.cs.unc.edu/~mbansal/teaching/nlp-course-fall17.html) [several](http://www.cs.unc.edu/~mbansal/teaching/nlp-seminar-spring18.html).\n",
    "\n",
    "- [Fantastic introduction to LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [Popular blog post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey of Machine Learning\n",
    "\n",
    "Question: Have we covered the major fields of ML?\n",
    "\n",
    "Answer: NLP, RL, Computer Vision are pretty major. Recommendation systems don't really fall into these buckets.\n",
    "\n",
    "For computer vision: we didn't go into all the subfields, just touched on classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
